{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "bd93f68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96691c46",
   "metadata": {},
   "source": [
    "## Data loading/proccessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fda4733",
   "metadata": {},
   "source": [
    "Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "2767cd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data with GA samples shape: (89, 995)\n",
      "data with FG samples shape: (569, 995)\n"
     ]
    }
   ],
   "source": [
    "data_ga = pd.read_csv(\"dataSrc/ga_2_3_5_6.csv\", sep=\";\")\n",
    "print(f\"data with GA samples shape: {data_ga.shape}\")\n",
    "data_fg = pd.read_csv(\"dataSrc/fg_1-5_7-11.csv\", sep=\";\")\n",
    "print(f\"data with FG samples shape: {data_fg.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "345efff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes of X and y of FG samples respectively is: (569, 994), (569,)\n",
      "shapes of X and y of GA samples respectively is: (89, 994), (89,)\n"
     ]
    }
   ],
   "source": [
    "y_fg = data_fg['class'].values\n",
    "X_fg = data_fg.drop(columns=['class'])\n",
    "print(f\"shapes of X and y of FG samples respectively is: {X_fg.shape}, {y_fg.shape}\")\n",
    "\n",
    "y_ga = data_ga['class'].values\n",
    "X_ga = data_ga.drop(columns=['class'])\n",
    "print(f\"shapes of X and y of GA samples respectively is: {X_ga.shape}, {y_ga.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbcbeaf",
   "metadata": {},
   "source": [
    "Наведем красоту:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "d0b1d490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old fg classes: {1, 2, 3, 4, 5, 7, 8, 9, 10, 11}\n",
      "new fg classes: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n",
      "\n",
      "old ga classes: {2, 3, 5, 6}\n",
      "new ga classes: {10, 11, 12, 13}\n"
     ]
    }
   ],
   "source": [
    "print(f\"old fg classes: {set(y_fg)}\")\n",
    "for index, element in enumerate(set(y_fg)):\n",
    "    y_fg[y_fg == element] = index\n",
    "print(f\"new fg classes: {set(y_fg)}\\n\")\n",
    "\n",
    "print(f\"old ga classes: {set(y_ga)}\")\n",
    "for index, element in enumerate(set(y_ga)):\n",
    "    y_ga[y_ga == element] = index + len(set(y_fg))\n",
    "print(f\"new ga classes: {set(y_ga)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6f19aa",
   "metadata": {},
   "source": [
    "Добавим к target(то есть, к y_fg и y_ga) группы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "19878187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now, shapes of y_fg and ga is respectively: (89, 2), (569, 2)\n"
     ]
    }
   ],
   "source": [
    "group1 = [0, 1, 2, 3, 4]\n",
    "group2 = [5, 6, 7, 8, 9]\n",
    "group3 = [10, 11]\n",
    "group4 = [12, 13]\n",
    "groups = [group1, group2, group3, group4]\n",
    "\n",
    "def add_group(y, groups, add=0):\n",
    "    new_y = np.zeros((y.shape[0], 2))\n",
    "    new_y[:, 1] = y\n",
    "    for group_num, group in enumerate(groups):\n",
    "        for class_num in group:\n",
    "            new_y[y==class_num] = np.array([group_num + add, class_num])\n",
    "    return new_y\n",
    "\n",
    "y_ga = add_group(y_ga, (group3, group4), add=2)\n",
    "y_fg = add_group(y_fg, (group1, group2))\n",
    "print(f\"Now, shapes of y_fg and ga is respectively: {y_ga.shape}, {y_fg.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0820ee06",
   "metadata": {},
   "source": [
    "Соединим две базы данных друг с другом: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "569d2a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes of X and y is respectively: (658, 1, 994), (658, 2)\n"
     ]
    }
   ],
   "source": [
    "y = np.concatenate((y_fg, y_ga), axis=0)\n",
    "X = np.concatenate((X_fg, X_ga), axis=0)\n",
    "X = X.reshape(X.shape[0], 1, -1)\n",
    "print(f\"shapes of X and y is respectively: {X.shape}, {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "3eec4553",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "dcf081f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_train = DataLoader(tuple(zip(torch.tensor(X_train).float(), torch.tensor(y_train).long())), \n",
    "                                      batch_size=32, \n",
    "                                      shuffle=True)\n",
    "data_loader_test = DataLoader(tuple(zip(torch.tensor(X_test).float(), torch.tensor(y_test).long())), \n",
    "                                     batch_size=32, \n",
    "                                     shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e229a59",
   "metadata": {},
   "source": [
    "Нарисуем один из спектров:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "6c4528de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(17, 9))\n",
    "# plt.plot(\n",
    "#     data.columns[1:-1].astype('float'), \n",
    "#     data.values[:, 1:-1][-1]\n",
    "# )\n",
    "# plt.grid()\n",
    "# plt.ylabel(\"Raman intensity\", fontsize=15)\n",
    "# plt.xlabel(\"Raman shift\", fontsize=15)\n",
    "# None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142307f7",
   "metadata": {},
   "source": [
    "## ResNet 1-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "14ce890f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class block1d(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels, \n",
    "        out_channels,\n",
    "        downsample=None,\n",
    "        stride=1,\n",
    "        kernel_size=3, \n",
    "        padding=1\n",
    "    ):\n",
    "        super(block1d, self).__init__()\n",
    "#         the last convolution outputs out_channels * self.expansion channels\n",
    "#         It is done in order to save number of parameters. \n",
    "#         self.expansion = dimension * 2\n",
    "        self.expansion = 2\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels, \n",
    "            out_channels,\n",
    "            kernel_size=1 ,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(\n",
    "            out_channels, \n",
    "            out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(\n",
    "            out_channels, \n",
    "            out_channels*self.expansion,\n",
    "            kernel_size=1 ,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn3 = nn.BatchNorm1d(out_channels*self.expansion)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "#         all the blocks in ResNet is devided by four groups, every next group is downsampled\n",
    "#         in relation to previous one. This field is responsible for that\n",
    "        self.downsample=downsample\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "#         in order to perform skip connection\n",
    "        identity = x.clone()\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(identity)\n",
    "        \n",
    "        x += identity\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "class ResNet1d(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        block,\n",
    "        layers,\n",
    "        num_classes,\n",
    "        num_groups\n",
    "    ):\n",
    "        super(ResNet1d, self).__init__()\n",
    "        self.expansion = 2\n",
    "        self.in_channels = 64\n",
    "        self.p = 0.2\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels=1,\n",
    "            out_channels=self.in_channels, \n",
    "            kernel_size=7, \n",
    "            stride=2, \n",
    "            padding=3, \n",
    "            bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.layer1 = self._make_layer(\n",
    "            block1d, layers[0], out_channels=64, stride=1\n",
    "        )\n",
    "        self.layer2 = self._make_layer(\n",
    "            block1d, layers[1], out_channels=128, stride=2\n",
    "        )\n",
    "        self.layer3 = self._make_layer(\n",
    "            block1d, layers[2], out_channels=256, stride=2\n",
    "        )\n",
    "        self.layer4 = self._make_layer(\n",
    "            block1d, layers[3], out_channels=512, stride=2\n",
    "        )\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc_class = nn.Sequential(\n",
    "            nn.Linear(32768, num_classes),\n",
    "            nn.Dropout(p=self.p)\n",
    "        )\n",
    "        self.fc_group = nn.Sequential(\n",
    "            nn.Linear(32768, num_groups),\n",
    "            nn.Dropout(p=self.p)\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "        \n",
    "    def _make_layer(self, block, num_residual_blocks, out_channels, stride):\n",
    "        downsample = None\n",
    "        layers = []\n",
    "        \n",
    "#         in order to adopt skip connection, we add downsample to the end of block group\n",
    "        if stride != 1 or self.in_channels != out_channels * self.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(\n",
    "                    self.in_channels, \n",
    "                    out_channels*self.expansion,\n",
    "                    kernel_size=1,\n",
    "                    stride=stride,\n",
    "                    bias=False\n",
    "                ),\n",
    "                nn.BatchNorm1d(out_channels*self.expansion)\n",
    "            )\n",
    "        \n",
    "        layers.append(\n",
    "            block1d(\n",
    "                self.in_channels, \n",
    "                out_channels, \n",
    "                downsample, \n",
    "                stride\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.in_channels = out_channels * self.expansion\n",
    "        \n",
    "        for i in range(num_residual_blocks - 1):\n",
    "            layers.append(\n",
    "                block1d(self.in_channels, out_channels)\n",
    "            )\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "#         x = self.avgpool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "#         print(x.shape)\n",
    "\n",
    "        groups = self.fc_group(x)\n",
    "        classes = self.fc_class(x)\n",
    "        \n",
    "        return (self.softmax(groups), self.softmax(classes))\n",
    "#         return (groups, classes)\n",
    "\n",
    "\n",
    "def ResNet50_1d(num_classes=14, num_groups=4):\n",
    "    return ResNet1d(block1d, [3, 4, 6, 3], num_classes, num_groups)\n",
    "\n",
    "\n",
    "def ResNet101_1d(num_classes=14, num_groups=4):\n",
    "    return ResNet1d(block1d, [3, 4, 23, 3], num_classes, num_groups)\n",
    "\n",
    "\n",
    "def ResNet152_1d(num_classes=14, num_groups=4):\n",
    "    return ResNet1d(block1d, [3, 8, 36, 3], num_classes, num_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "e3e090d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test():\n",
    "#     net = ResNet101_1d(num_classes=14, num_groups=4)\n",
    "#     y = net(torch.randn(4, 1, 994))\n",
    "#     return y\n",
    "\n",
    "\n",
    "# y = test()\n",
    "\n",
    "# y[0].size(), y[1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "d6f602e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(data_loader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "51502c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_func, epochs, train_loader, val_loader):\n",
    "    train_history = []\n",
    "    test_history = []\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        avg_train_loss = 0\n",
    "        avg_test_loss = 0\n",
    "        right_preds_group = 0\n",
    "        right_preds_classes = 0\n",
    "        size_group = 0\n",
    "        size_classes = 0\n",
    "        for X_batch, y_batch in tqdm(train_loader):\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs_groups, outputs_classes = model(X_batch)\n",
    "#             print(y_batch[:, 0].shape)\n",
    "#             print(outputs_groups.shape)\n",
    "            loss_group = loss_func(outputs_groups, y_batch[:, 0])\n",
    "            loss_classes = loss_func(outputs_classes, y_batch[:, 1])\n",
    "            loss = loss_group + loss_classes\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad\n",
    "            avg_train_loss += loss / len(train_loader)\n",
    "            \n",
    "            y_pred_group = torch.argmax(outputs_groups, dim = 1)\n",
    "            right_preds_group += torch.sum(y_pred_group == y_batch[:, 0])\n",
    "            size_group += y_pred_group.size()[0]\n",
    "            \n",
    "            y_pred_classes = torch.argmax(outputs_classes, dim = 1)\n",
    "#             print(y_pred_classes,y_batch[:, 1])\n",
    "            right_preds_classes += torch.sum(y_pred_classes == y_batch[:, 1])\n",
    "            size_classes += y_pred_classes.size()[0]\n",
    "            \n",
    "        train_acc_group = right_preds_group / size_group\n",
    "        train_acc_classes = right_preds_classes / size_classes\n",
    "        train_history.append(avg_train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        right_preds_group = 0\n",
    "        right_preds_classes = 0\n",
    "        size_group = 0\n",
    "        size_classes = 0\n",
    "        for X_batch, y_batch in tqdm(val_loader):\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs_groups, outputs_classes = model(X_batch)\n",
    "                loss_group = loss_func(outputs_groups, y_batch[:, 0])\n",
    "                loss_classes = loss_func(outputs_classes, y_batch[:, 1])\n",
    "                loss = loss_group + loss_classes\n",
    "    # outputs неправильный!!\n",
    "                avg_test_loss += loss / len(val_loader)\n",
    "#                 y_pred = torch.argmax(outputs, dim = 1)\n",
    "#                 right_preds += torch.sum(y_pred == y_batch)\n",
    "#                 size += y_pred.size()[0]\n",
    "#         test_acc = right_preds / size\n",
    "                y_pred_group = torch.argmax(outputs_groups, dim = 1)\n",
    "                right_preds_group += torch.sum(y_pred_group == y_batch[:, 0])\n",
    "                size_group += y_pred_group.size()[0]\n",
    "\n",
    "                y_pred_classes = torch.argmax(outputs_classes, dim = 1)\n",
    "                right_preds_classes += torch.sum(y_pred_classes == y_batch[:, 1])\n",
    "                size_classes += y_pred_classes.size()[0]\n",
    "            \n",
    "        test_acc_group = right_preds_group / size_group\n",
    "        test_acc_classes = right_preds_classes / size_classes\n",
    "        \n",
    "        test_history.append(avg_test_loss)\n",
    "        print(f\"\"\"epochs: {epoch + 1}/{epochs}, \n",
    "        train loss: {round(avg_train_loss.item(), 3)}, test loss: {round(avg_test_loss.item(), 3)}\n",
    "        train group accuracy: {round(train_acc_group.item() * 100, 2)}, test group accuracy: {round(test_acc_group.item() * 100, 2)}\n",
    "        train class accuracy: {round(train_acc_classes.item() * 100, 2)}, test class accuracy: {round(test_acc_classes.item() * 100, 2)}\"\"\")\n",
    "        torch.cuda.empty_cache()\n",
    "#     \n",
    "    return train_history, test_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "df7ec5a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2c4dabfa918426fb7f985e0f495f8b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf186a26dde34f848eef840a40683714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd8537c34a0e4e108fa4f73b49860ae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 1/10, \n",
      "        train loss: 3.559, test loss: 3.91\n",
      "        train group accuracy: 76.05, test group accuracy: 54.55\n",
      "        train class accuracy: 17.87, test class accuracy: 9.85\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2503e96d39e46b5a2994e71b52d21d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85cd4d80cf4c4317a6f6b93bbf7a4b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 2/10, \n",
      "        train loss: 3.508, test loss: 3.998\n",
      "        train group accuracy: 84.41, test group accuracy: 34.85\n",
      "        train class accuracy: 15.4, test class accuracy: 9.85\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "986c89e3448b4f78a91fac4e894ea411",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8901791a70284f0e9e6aa3b58aa33e46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 3/10, \n",
      "        train loss: 3.523, test loss: 3.461\n",
      "        train group accuracy: 84.22, test group accuracy: 89.39\n",
      "        train class accuracy: 15.97, test class accuracy: 15.15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa34ddda45844a35bc015bf01736233a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2f07bc9f04b4ac6b5138b3dff37cded",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 4/10, \n",
      "        train loss: 3.513, test loss: 3.461\n",
      "        train group accuracy: 84.6, test group accuracy: 89.39\n",
      "        train class accuracy: 15.21, test class accuracy: 15.15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "827c04566f9b47b5b80abacdf12a18bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2beceb175f794a2baa0127c8a48c460c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 5/10, \n",
      "        train loss: 3.486, test loss: 3.461\n",
      "        train group accuracy: 85.74, test group accuracy: 89.39\n",
      "        train class accuracy: 16.92, test class accuracy: 15.15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4673ae4f2c7437cae69d3f0fe82e94e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ac99f33c6eb41809e1ff42a4fc3001c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 6/10, \n",
      "        train loss: 3.5, test loss: 3.461\n",
      "        train group accuracy: 85.36, test group accuracy: 89.39\n",
      "        train class accuracy: 15.78, test class accuracy: 15.15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eb0b40dcb334269a4286263c2eccf34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0978c0b969234deabb76754819992cc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 7/10, \n",
      "        train loss: 3.498, test loss: 3.461\n",
      "        train group accuracy: 85.93, test group accuracy: 89.39\n",
      "        train class accuracy: 16.54, test class accuracy: 15.15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dfbab22b4444fbf86762b6944605316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-284-193d56ca9571>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mloss_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmax_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m train_hist, test_hist = train(\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-283-cf4064c7ead4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, loss_func, epochs, train_loader, val_loader)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mloss_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_group\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = ResNet50_1d().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-4, weight_decay = 0.01)\n",
    "# scheduler = optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=0.75)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "max_epochs = 10\n",
    "train_hist, test_hist = train(\n",
    "    model, \n",
    "    optimizer,\n",
    "    loss_function,\n",
    "    max_epochs,\n",
    "    data_loader_train, \n",
    "    data_loader_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "5bae63bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0699,  1.3486,  0.5678, -0.6970, -0.0530],\n",
      "        [ 1.6018,  0.8620,  0.7413,  0.7008,  0.5210],\n",
      "        [-0.1126,  0.7997, -0.9675,  0.4259, -1.2378],\n",
      "        [-2.3615,  1.8797,  1.3781,  1.1282, -0.3959],\n",
      "        [-0.4266,  0.4714,  0.4549, -0.8283, -0.0751],\n",
      "        [ 0.7394,  1.3619, -0.3950, -0.2331, -0.5946],\n",
      "        [ 0.4697, -1.5694,  0.1416, -0.0160,  0.2702],\n",
      "        [ 0.8575, -1.3289,  2.1843, -0.3127,  0.7848],\n",
      "        [ 0.0844, -0.7548, -0.8881, -0.0707,  0.0361],\n",
      "        [-0.8675, -0.3160, -0.3415,  0.0525, -0.3403]], requires_grad=True) tensor([0, 4, 4, 3, 2, 0, 4, 2, 3, 0])\n"
     ]
    }
   ],
   "source": [
    "# Example of target with class indices\n",
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(10, 5, requires_grad=True)\n",
    "target = torch.empty(10, dtype=torch.long).random_(5)\n",
    "output = loss(input, target)\n",
    "# output.backward()\n",
    "print(input, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d5aaf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
